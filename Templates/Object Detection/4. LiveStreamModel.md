# About

This will be a simplified tutorial that assumes you are already very familiar with your dataset and your model. The intention is to help you quickly set up a system that allows you to live-stream your model using a webcam and run real time object detection. As with the other tutorials in this section we will continue to use PyTorch.

# Starter Imports

```python
# Model
import torch
from torchvision.models import detection
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor

# Video
from imutils.video import VideoStream # Access webcam
from imutils.video import FPS # Track FPS from webcam
import time
import cv2

# Dataset Manipulation
from PIL import Image
import numpy as np
```

# Set Device Type

```python
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
print(device)
```

# Initialize Model

```python
# Create list of classes/labels you are working with in your dataset
CLASSES = ['person', 'fish']

# Create model switch
MODELS = {
	"frcnn-resnet": detection.fasterrcnn_resnet50_fpn,
    "frcnn-mobilenet": detection.fasterrcnn_mobilenet_v3_large_320_fpn,
}

# Initialize the pytorch model
model = MODELS["frcnn-mobilenet"](weights="DEFAULT").to(device)
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = FastRCNNPredictor(in_features, len(CLASSES))

# Load local fine-tuned weights
model.load_state_dict(torch.load('CardWeights.pt'))
model.to(device)

# Set to evaluation mode
model.eval()
print("Success")
```

# Initialize Video Stream

```python
# Initialize the video stream, allow the camera sensor to warmup, and initialize the FPS counter
print("[INFO] starting video stream...")
vs = VideoStream(src=0).start() # Set src to your webcam, generally this is 0
time.sleep(2.0)
fps = FPS().start()
```

# Run Object Detection

```python
# set of bounding box colors for each class
COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))

# loop over the frames from the video stream
cv2.namedWindow("Frame")
cv2.setMouseCallback("Frame", click_event)
last_click = click_coordinates[-1]
while True:
	# grab the frame from the threaded video stream and resize it
	# to have a maximum width of 400 pixels
	frame = vs.read()
	# frame = imutils.resize(frame, width=400)
	orig = frame.copy()
	# convert the frame from BGR to RGB channel ordering and change
	# the frame from channels last to channels first ordering
	frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
	frame = frame.transpose((2, 0, 1))
	# add a batch dimension, scale the raw pixel intensities to the
	# range [0, 1], and convert the frame to a floating point tensor
	frame = np.expand_dims(frame, axis=0)
	frame = frame / 255.0
	frame = torch.FloatTensor(frame)
	# send the input to the device and pass the it through the
	# network to get the detections and predictions
	frame = frame.to(device)
	detections = model(frame)[0]

	# loop over the detections
	for i in range(0, len(detections["boxes"])):
		# extract the confidence (i.e., probability) associated with
		# the prediction
		confidence = detections["scores"][i]
		# filter out weak detections by ensuring the confidence is
		# greater than the minimum confidence
		if confidence > .8:
			# extract the index of the class label from the
			# detections, then compute the (x, y)-coordinates of
			# the bounding box for the object
            idx = int(detections['labels'][i])
			box = detections["boxes"][i].detach().cpu().numpy()
			(startX, startY, endX, endY) = box.astype("int")
			# draw the bounding box and label on the frame
			label = "{}: {:.2f}%".format(CLASSES[idx], confidence * 100)
			cv2.rectangle(orig, (startX, startY), (endX, endY), COLORS[idx], 2)
			y = startY - 15 if startY - 15 > 15 else startY + 15
			cv2.putText(orig, label, (startX, y),
				cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2)
		
	# show the output frame
	cv2.imshow("Frame", orig)
	key = cv2.waitKey(1) & 0xFF
	# if the 'q' key was pressed, break from the loop
	if key == ord("q"):
		break
	# update the FPS counter
	fps.update()

# stop the timer and display FPS information
fps.stop()
print("[INFO] elapsed time: {:.2f}".format(fps.elapsed()))
print("[INFO] approx. FPS: {:.2f}".format(fps.fps()))
# do a bit of cleanup
cv2.destroyAllWindows()
vs.stop()
```