{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrcNNEcOS30q"
      },
      "source": [
        "\n",
        "# [Download kaggle dataset](https://www.kaggle.com/datasets/andrewmvd/face-mask-detection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YY9Wm1bCQDV4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Set up Kaggle dataset\n",
        "! kaggle datasets download -d andrewmvd/face-mask-detection\n",
        "! unzip face-mask-detection.zip -d face-mask-detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivvd464JS8rh"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VTbdLZKS3VK"
      },
      "outputs": [],
      "source": [
        "# Model Building\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets, models\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Data Extraction\n",
        "from bs4 import BeautifulSoup # Parse through xml files and extract bounding boxes / labels\n",
        "from PIL import Image # Image editing\n",
        "\n",
        "# Data Visualization\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Working with files and model saves\n",
        "import os\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z4jY7y9T8b6"
      },
      "source": [
        "# Functions to parse xml data and retrieve bounding boxes + labels\n",
        "\n",
        "\n",
        "Following code snippet taken from Pytorch FasterRCNN by Daniel and tweaked to match this project\n",
        "\n",
        "*   def generate_box(obj): Input \"object\" from xml; Output bounding box\n",
        "*   def generate_label(obj): Input \"object\" from xml; Output label\n",
        "\n",
        "\n",
        "Fine-tuning pretrained fasterrcnn_resnet50_fpn requires the following input tensors and targets:\n",
        "\n",
        "*   Bounding-boxes of type FloatTensor[N, 4]\n",
        "*   Labels of type Int64Tensor[N]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1KzO-3__THVh"
      },
      "outputs": [],
      "source": [
        "# Get Ground Truth Box\n",
        "def generate_box(obj):\n",
        "    xmin = int(obj.find('xmin').text)\n",
        "    ymin = int(obj.find('ymin').text)\n",
        "    xmax = int(obj.find('xmax').text)\n",
        "    ymax = int(obj.find('ymax').text)\n",
        "\n",
        "    return [xmin, ymin, xmax, ymax]\n",
        "\n",
        "# Get Ground Truth Label\n",
        "def generate_label(obj):\n",
        "    if obj.find('name').text == \"with_mask\":\n",
        "        return 1\n",
        "    elif obj.find('name').text == \"mask_weared_incorrect\": # Yes, this is how it is labeled in the xml file\n",
        "        return 2\n",
        "    return 0 # without_mask\n",
        "\n",
        "# Use BeautifulSoup to read xml data, all faces stored in objects[]\n",
        "def generate_target(file):\n",
        "    with open(file) as f:\n",
        "        data = f.read()\n",
        "        soup = BeautifulSoup(data, 'xml')\n",
        "        objects = soup.find_all('object')\n",
        "\n",
        "        # Bounding boxes for objects\n",
        "        # In pytorch, the input should be [xmin, ymin, xmax, ymax], WE ARE USING THIS\n",
        "        # In coco format, bounding box = [xmin, ymin, width, height]\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for obj in objects:\n",
        "            boxes.append(generate_box(obj))\n",
        "            labels.append(generate_label(obj))\n",
        "\n",
        "        # Convert boxes and labels to tensor objects for fasterrcnn_resnet50_fpn\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        # Annotation is in dictionary format\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        return target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bgz-odCudbl"
      },
      "source": [
        "# CREATE DATALOADER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guDSC_ffqx6W"
      },
      "source": [
        "## Create Dataset() class for torch.utils.data.DataLoader(), this is a Map-style Dataset\n",
        "\n",
        "*   implements \\__getitem__() and \\__len__()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "80ZcFU50pPqJ"
      },
      "outputs": [],
      "source": [
        "class Dataset():\n",
        "  def __init__(self, indices):\n",
        "    self.imgs = list(sorted(os.listdir(\"/content/face-mask-detection/images/\")))\n",
        "    self.labels = list(sorted(os.listdir(\"/content/face-mask-detection/annotations/\")))\n",
        "    self.indices = indices # Used for train/validation split\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # Generate file paths from idx\n",
        "    img_file = 'maksssksksss'+ str(self.indices[idx]) + '.png'\n",
        "    label_file = 'maksssksksss'+ str(self.indices[idx]) + '.xml'\n",
        "\n",
        "    img_path = os.path.join(\"/content/face-mask-detection/images\", img_file)\n",
        "    label_path = os.path.join(\"/content/face-mask-detection/annotations\", label_file)\n",
        "\n",
        "    # Load Image\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    data_transform = transforms.Compose([transforms.ToTensor(), ])\n",
        "    img = data_transform(img)\n",
        "\n",
        "    # Generate Label\n",
        "    target = generate_target(label_path)\n",
        "\n",
        "    return img, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifds5BAFiI_i"
      },
      "source": [
        "## Create collate function for torch.utils.data.DataLoader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gWc9e6GciEaP"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "  return tuple(zip(*batch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciiBfK3eb0FI"
      },
      "source": [
        "## Train/Valid Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KaZyzAPbwqf6"
      },
      "outputs": [],
      "source": [
        "total_imgs = len(list(sorted(os.listdir(\"/content/face-mask-detection/images/\"))))\n",
        "train_indices, valid_indices = train_test_split(range(total_imgs), test_size=0.3, random_state=1) # I have used random_state=1 for training this model (reproducibility purposes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwgcfpzIwq34"
      },
      "source": [
        "## Create train and validation DataLoader() objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TK-w284UbymJ"
      },
      "outputs": [],
      "source": [
        "# IIf using cpu on colab free, 16/8 batch_size maximizes memory and training speeds\n",
        "train_dl = torch.utils.data.DataLoader(dataset=Dataset(train_indices),\n",
        "                                          batch_size=32,\n",
        "                                          collate_fn=collate_fn)\n",
        "\n",
        "valid_dl = torch.utils.data.DataLoader(dataset=Dataset(valid_indices),\n",
        "                                          batch_size=16,\n",
        "                                          collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNC8F_XfU-kg"
      },
      "source": [
        "# Make instance of pretrained Faster R-CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QgbcyXa4VFbx"
      },
      "outputs": [],
      "source": [
        "def get_model(num_classes):\n",
        "  model = models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBOv9cHnzRJ6",
        "outputId": "532c554a-2650-4031-f913-e0bafb0ff945"
      },
      "outputs": [],
      "source": [
        "# Instantiate\n",
        "model = get_model(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lN8LXtKVF6Y"
      },
      "source": [
        "# Fine-tune Faster R-CNN model on our dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwMMwmgIzdZI"
      },
      "source": [
        "## Create optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0pfGz6jKVKbd"
      },
      "outputs": [],
      "source": [
        "# For this project I did not experiment with various optimizer settings\n",
        "optimizer = torch.optim.SGD(model.parameters(),\n",
        "                            lr=0.001,\n",
        "                            momentum=0.9,\n",
        "                            weight_decay=0.0005)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQYMm3E50IQN"
      },
      "source": [
        "## Set Device type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1vtzmHcz8qm",
        "outputId": "d45a4a38-744b-46b3-b21b-79ee5a2642ea"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCBCFZkcbwCW"
      },
      "source": [
        "## LOAD MODEL FROM GDRIVE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgQ-QPYObvo3",
        "outputId": "2fce08b2-f808-4011-8fc9-8f602a1b8c72"
      },
      "outputs": [],
      "source": [
        "# Google drive must be mounted beforehand\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/{YourPathHere}.pt'))\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewJ4lrmu0Pjf"
      },
      "source": [
        "## Fine-tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDFF3O6d0Hyu",
        "outputId": "b6058f76-b706-4de1-c533-c2dbbe21ea25"
      },
      "outputs": [],
      "source": [
        "model.to(device) # In the event you don't load a saved model use this\n",
        "num_epochs = 10\n",
        "len_dataloader = len(train_dl)\n",
        "\n",
        "for epochs in range(num_epochs):\n",
        "  model.train()\n",
        "  i = 0\n",
        "  epoch_loss = 0\n",
        "  for imgs, annotations in train_dl:\n",
        "    i += 1\n",
        "    imgs = list(img.to(device) for img in imgs)\n",
        "    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
        "\n",
        "    loss_dict = model([imgs[0]], [annotations[0]])\n",
        "    loss = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print(f'Iteration: {i}/{len_dataloader}, Loss: {loss}') # Show each iteration\n",
        "    epoch_loss += loss\n",
        "\n",
        "  print(epoch_loss)\n",
        "  torch.save(model.state_dict(),'/content/drive/MyDrive/{YourPathHere}.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "B4lKeh9GPhNH"
      },
      "outputs": [],
      "source": [
        "# Prepare testing on validation set\n",
        "for imgs, annotations in valid_dl:\n",
        "      imgs = list(img.to(device) for img in imgs)\n",
        "      annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
        "      break # Take only first batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enter evaluation mode and generate predictions of validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmBaLxSWPu9p",
        "outputId": "dbd859f3-2cca-4438-e3a9-e3be4fa62568"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "# Use with torch.no_grad() to prevent memory buildup exceeding colab's limits when running on cpu\n",
        "# If still exceeding reduce validation batch_size\n",
        "with torch.no_grad():\n",
        "  preds = model(imgs) # Generate predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "77D572cpP13q"
      },
      "outputs": [],
      "source": [
        "def plot_image(img_tensor, annotation):\n",
        "\n",
        "    fig,ax = plt.subplots(1)\n",
        "    img = img_tensor.cpu().data\n",
        "\n",
        "    # Display the image\n",
        "    ax.imshow(img.permute(1, 2, 0))\n",
        "\n",
        "    for box in annotation[\"boxes\"]:\n",
        "        box = box.cpu() # Use if running cuda\n",
        "        xmin, ymin, xmax, ymax = box.detach().numpy()\n",
        "\n",
        "        # Create a Rectangle patch\n",
        "        rect = patches.Rectangle((xmin.item(),ymin.item()),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
        "\n",
        "        # Add the patch to the Axes\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "id": "yp8ZvSG1P7yf",
        "outputId": "9bcedb5b-1952-4184-fd94-96764caa38d1"
      },
      "outputs": [],
      "source": [
        "IMG_NUM = 4 # [0, batch_size - 1]\n",
        "print(\"Prediction\")\n",
        "plot_image(imgs[IMG_NUM], preds[IMG_NUM])\n",
        "print(\"Target\")\n",
        "plot_image(imgs[IMG_NUM], annotations[IMG_NUM])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prediction: Left | \n",
        "Target: Right |\n",
        "Epochs Trained: 300 |\n",
        "Batch_Size: 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Predict](Images/Predict_Facemasks.png)\n",
        "![Target](Images/Target_Facemasks.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8mTLQCZbpyY"
      },
      "source": [
        "# SAVE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHG9ioBRLXJs"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(),'/content/drive/MyDrive/{YourPathHere}.pt') # Make sure GoogleDrive is mounted beforehand"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "y8mTLQCZbpyY"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
